<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Harlinn.AI DirectML (DML) | Harlinn</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Harlinn.AI DirectML (DML)" />
<meta name="author" content="Espen Harlinn" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is my personal site." />
<meta property="og:description" content="This is my personal site." />
<link rel="canonical" href="https://harlinn.github.io/Cpp/Harlinn.Windows/Harlinn.AI/Harlinn.AI.DML.html" />
<meta property="og:url" content="https://harlinn.github.io/Cpp/Harlinn.Windows/Harlinn.AI/Harlinn.AI.DML.html" />
<meta property="og:site_name" content="Harlinn" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Harlinn.AI DirectML (DML)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Espen Harlinn"},"description":"This is my personal site.","headline":"Harlinn.AI DirectML (DML)","url":"https://harlinn.github.io/Cpp/Harlinn.Windows/Harlinn.AI/Harlinn.AI.DML.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://harlinn.github.io/feed.xml" title="Harlinn" /><script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="importmap">
        {
            "imports":
            {
            "three": "https://cdn.jsdelivr.net/npm/three@0.171.0/build/three.module.min.js" ,
            "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.171.0/examples/jsm/"
            }
        }
    </script>
</head>
<body><header class="site-header" role="banner">
    <div id="animatedHeader" height="120px" width="100%">
        <script type="x-shader/x-vertex" id="vertexshader">
            attribute float scale;
            void main() {
                vec4 mvPosition = modelViewMatrix * vec4( position, 1.0 );
                gl_PointSize = scale * ( 100.0 / - mvPosition.z );
                gl_Position = projectionMatrix * mvPosition;
            }

        </script>

        <script type="x-shader/x-fragment" id="fragmentshader">
            uniform vec3 color;
            void main() {
                if ( length( gl_PointCoord - vec2( 0.5, 0.5 ) ) > 0.475 )  discard;
                gl_FragColor = vec4( color, 0.8 );
            }
        </script>

        <script type="module">

			import * as THREE from 'three';

			const SEPARATION = 100, AMOUNTX = 50, AMOUNTY = 50;

			let container; 
			let camera, scene, renderer;

			let particles, count = 0;

			let mouseX = 0, mouseY = 0;

			let animatedHeaderHeight = 120;

            let windowHalfX = window.innerWidth / 2;
            let windowHalfY = animatedHeaderHeight/2.0;

			init();

			function init() {

                container = document.querySelector('#animatedHeader');

                camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 1, 10000 );
				camera.position.z = 1000;

				scene = new THREE.Scene();

				//

				const numParticles = AMOUNTX * AMOUNTY;

				const positions = new Float32Array( numParticles * 3 );
				const scales = new Float32Array( numParticles );

				let i = 0, j = 0;

				for ( let ix = 0; ix < AMOUNTX; ix ++ ) {

					for ( let iy = 0; iy < AMOUNTY; iy ++ ) {

						positions[ i ] = ix * SEPARATION - ( ( AMOUNTX * SEPARATION ) / 2 ); // x
						positions[ i + 1 ] = 0; // y
						positions[ i + 2 ] = iy * SEPARATION - ( ( AMOUNTY * SEPARATION ) / 2 ); // z

						scales[ j ] = 1;

						i += 3;
						j ++;

					}

				}

				const geometry = new THREE.BufferGeometry();
				geometry.setAttribute( 'position', new THREE.BufferAttribute( positions, 3 ) );
				geometry.setAttribute( 'scale', new THREE.BufferAttribute( scales, 1 ) );

				const material = new THREE.ShaderMaterial( {

					uniforms: {
						color: { value: new THREE.Color( 0xffffff ) },
					},
					vertexShader: document.getElementById( 'vertexshader' ).textContent,
					fragmentShader: document.getElementById( 'fragmentshader' ).textContent

				} );

				//

				particles = new THREE.Points( geometry, material );
				scene.add( particles );

				//

				renderer = new THREE.WebGLRenderer( { antialias: true } );
				renderer.setPixelRatio( window.devicePixelRatio );
                renderer.setSize(window.innerWidth, animatedHeaderHeight );
				renderer.setAnimationLoop( animate );
				container.appendChild( renderer.domElement );

				container.style.touchAction = 'none';
				container.addEventListener( 'pointermove', onPointerMove );

				//

                window.addEventListener( 'resize', onWindowResize );

			}

			function onWindowResize() {

				windowHalfX = window.innerWidth / 2;

				camera.aspect = window.innerWidth / animatedHeaderHeight; 
				camera.updateProjectionMatrix();

				renderer.setSize(window.innerWidth, animatedHeaderHeight);

			}

			//

			function onPointerMove( event ) {

				if ( event.isPrimary === false ) return;

				mouseX = event.clientX - windowHalfX;
				mouseY = event.clientY - windowHalfY;

			}

			//

			function animate() {
				render();

			}

			function render() {

				camera.position.x += ( mouseX - camera.position.x ) * .05;
				camera.position.y += ( - mouseY - camera.position.y ) * .05;
				camera.lookAt( scene.position );

				const positions = particles.geometry.attributes.position.array;
				const scales = particles.geometry.attributes.scale.array;

				let i = 0, j = 0;

				for ( let ix = 0; ix < AMOUNTX; ix ++ ) {

					for ( let iy = 0; iy < AMOUNTY; iy ++ ) {

						positions[ i + 1 ] = ( Math.sin( ( ix + count ) * 0.3 ) * 50 ) +
										( Math.sin( ( iy + count ) * 0.5 ) * 50 );

						scales[ j ] = ( Math.sin( ( ix + count ) * 0.3 ) + 1 ) * 20 +
										( Math.sin( ( iy + count ) * 0.5 ) + 1 ) * 20;

						i += 3;
						j ++;

					}

				}

				particles.geometry.attributes.position.needsUpdate = true;
				particles.geometry.attributes.scale.needsUpdate = true;

				renderer.render( scene, camera );

				count += 0.1;

			}

        </script>

    </div>
  <div class="wrapper"><a class="site-title" rel="author" href="/">Harlinn</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Cpp/Cpp.html">C++</a><a class="page-link" href="/Databases/Databases.html">Databases</a><a class="page-link" href="/DotNet/DotNet.html">.Net</a><a class="page-link" href="/Java/Java.html">Java</a><a class="page-link" href="/Julia/Julia.html">Julia</a><a class="page-link" href="/Python/Python.html">Python</a><a class="page-link" href="/R/R.html">R</a><a class="page-link" href="/Rust/Rust.html">Rust</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Harlinn.AI DirectML (DML)</h1>
  </header>

  <div class="post-content">
    <p>The DirectML wrappers are a key component of Harlinn.AI, providing an API on top 
of the DirectML API, facilitating robust C++ DirectML development.</p>

<p><img src="/Cpp/Harlinn.Windows/Examples/AI\DirectML/Yolo9/ObjectDetection3.png" alt="Yolo v9" /></p>

<p>DirectML is a high-performance, hardware-accelerated DirectX 12 library for 
executing AI workloads. DirectML provides hardware acceleration for common AI 
tasks across a broad range of hardware, including neural processing units (NPU) 
and DirectX 12-capable GPUs.</p>

<p>NPUs are processing units that simulates the neural network of the brain, capable of
processing large amounts of data in parallel, performing trillions of operations per 
second, providing a cost efficient alternative for AI workloads.</p>

<p>Using DirectML we can create hardware accelerated AI apps capable of leveraging 
the capabilities of the system for executing those workloads efficiently, regardless of whether
they run on GPUs or NPUs.</p>

<p>DirectML leverages the Direct3D 12 execution model, where efficient execution of AI
workloads can be interleaved with execution of graphic workloads. For now, this
is an important feature of DirectML, as the current generation of NPUs lacks the
processing power of high end GPUs.</p>

<p>DirectML executes hardware-accelerated AI primitives, called operators, on a suitable 
device on the system running the AI workload. Operators are building blocks that 
can be executed individually, or composed into a graph that fully describes
an AI task that can be compiled and executed by DirectML.</p>

<p>Harlinn.AI redeclares the description operator structures provided with DirectML,
providing a complete set of new structures describing all the operators implemented
by DirectML. Every operator describing structure in the <code class="language-plaintext highlighter-rouge">Harlinn::AI::DML</code> namespace
is binary compatible with the original structure from DirectML describing a particular
operator, but with some key differences:</p>

<ol>
  <li>A <code class="language-plaintext highlighter-rouge">static constexpr DML::OperatorType OperatorType</code> identifying the operator.</li>
  <li>All operator describing structures are directly, or indirectly, derived from <code class="language-plaintext highlighter-rouge">struct BaseOperatorDesc</code>.</li>
  <li>Every operator member variable gets initialized to a default value.</li>
  <li>The operator describing structures have constructors accepting similar arguments, in the same order, even when
the struct declares its members in a different order.</li>
  <li>Nearly all unary operators are derived from <code class="language-plaintext highlighter-rouge">struct UnaryOperatorDesc</code>. The exceptions are the
unary operators defined by DirectML that have an incompatible memory layout.</li>
  <li>Nearly all binary operators are derived from <code class="language-plaintext highlighter-rouge">struct BinaryOperatorDesc</code>. The exceptions are the
binary operators defined by DirectML that have an incompatible memory layout.</li>
</ol>

<p>These features makes it easier to create template based code in C++ for the operators.</p>

<p>This is used by <a href="Harlinn.AI.DML.X.html">Harlinn.AI DirectML eXtensions (DML.X)</a> to implement
a framework that makes it easier to compose a graph of operators that describes an AI task
that can be compiled and executed by DirectML.</p>

<p>The DirectML wrappers can also be used with the <a href="https://www.nuget.org/packages/Microsoft.AI.MachineLearning/">Microsoft.AI.MachineLearning</a> package,
which is a cross-platform library that supports the open standard ONNX format 
for machine learning models. The ONNX Runtime can use DirectML as one of its 
execution providers, along with other backends such as CPU, CUDA, or TensorRT.</p>

<h3 id="operator-descriptors">Operator Descriptors</h3>

<p>Operator descriptors are used to define the datatype and shape of input and output
tensors. In many cases the operator descriptors also hold additional parameters 
for the operation they describe.</p>

<h4 id="baseoperatordesc">BaseOperatorDesc</h4>

<p>All operator descriptors are derived from <code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code>, an empty struct
adding zero bytes to the derived operator descriptors due to <a href="https://en.cppreference.com/w/cpp/language/ebo">Empty base optimization</a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>struct BaseOperatorDesc abstract
{
};
</code></pre></div></div>

<h4 id="unaryoperatordesc">UnaryOperatorDesc</h4>

<p>Nearly all unary DirectML operators have <code class="language-plaintext highlighter-rouge">const TensorDesc* InputTensor</code> and <code class="language-plaintext highlighter-rouge">const TensorDesc* OutputTensor</code> as
the first two member fields of their descriptor type. When this is the case, Harlinn.AI DirectML
derives the operator descriptor from <code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code>.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="unaryoperatorwithscalebiasdesc">UnaryOperatorWithScaleBiasDesc</h4>

<p>Many unary DirectML operators have an optional <code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code> as the third member
field of their descriptor type. When this is the case, Harlinn.AI DirectML
derives the operator descriptor from <code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code>.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<h4 id="binaryoperatordesc">BinaryOperatorDesc</h4>

<p>Many binary DirectML operators have <code class="language-plaintext highlighter-rouge">const TensorDesc* ATensor</code>, <code class="language-plaintext highlighter-rouge">const TensorDesc* BTensor</code> and <code class="language-plaintext highlighter-rouge">const TensorDesc* OutputTensor</code> as
the first three member fields of their descriptor type. When this is the case, Harlinn.AI DirectML
derives the operator descriptor from <code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code>.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="elementwiseidentityoperatordesc">ElementWiseIdentityOperatorDesc</h4>

<p>Computes the identity for each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing 
the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x)=x\]

<p>or</p>

\[f(x)=x \times scale + bias\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiseabsoperatordesc">ElementWiseAbsOperatorDesc</h4>

<p>Computes the absolute value for each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing 
the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x)=|x|\]

<p>or</p>

\[f(x)=|x \times scale + bias|\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiseacosoperatordesc">ElementWiseACosOperatorDesc</h4>

<p>Computes the inverse cosine of each element of InputTensor, placing 
the result into the corresponding element of OutputTensor.</p>

\[f(x)=cos^{-1}(x)\]

<p>or</p>

\[f(x)=cos^{-1}(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiseaddoperatordesc">ElementWiseAddOperatorDesc</h4>

<p>Adds every element in <code class="language-plaintext highlighter-rouge">ATensor</code> to its corresponding element in <code class="language-plaintext highlighter-rouge">BTensor</code>, placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)=a+b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, or both, during binding.</p>

<h4 id="elementwiseadd1operatordesc">ElementWiseAdd1OperatorDesc</h4>

<p>Adds every element in ATensor to its corresponding element in BTensor and places the result 
into the corresponding element of OutputTensor, with the option for fused activation.</p>

\[f(a,b)=a+b\]

<p>or</p>

\[f(a,b)=FusedActivation(a+b)\]

<p>when <code class="language-plaintext highlighter-rouge">fusedActivation</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">fusedActivation</code></td>
      <td><code class="language-plaintext highlighter-rouge">const OperatorDesc*</code></td>
      <td>An optional operator descriptor for a fused activation operator to be applied to the output.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, or both, during binding.</p>

<h4 id="elementwiseasinoperatordesc">ElementWiseASinOperatorDesc</h4>

<p>Computes the arcsine for each element of InputTensor, placing the result 
into the corresponding element of OutputTensor.</p>

\[f(x)=sin^{-1}(x)\]

<p>or</p>

\[f(x)=sin^{-1}(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiseatanoperatordesc">ElementWiseATanOperatorDesc</h4>

<p>Computes the arctangent for each element of InputTensor, 
placing the result into the corresponding element of OutputTensor.</p>

\[f(x)=tan^{-1}(x)\]

<p>or</p>

\[f(x)=tan^{-1}(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiseceiloperatordesc">ElementWiseCeilOperatorDesc</h4>

<p>Computes the ceiling for each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing 
the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>. 
The ceiling of \(x\) is the smallest integer that is greater than or equal to \(x\).</p>

\[f(x)=ceil(x)\]

<p>or</p>

\[f(x)=ceil(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiseclipoperatordesc">ElementWiseClipOperatorDesc</h4>

<p>Performs the following operation for each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing 
the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>. This operator 
clamps (or limits) every element in the input within the closed interval \([Min, Max]\).</p>

\[f(x)=max(Min,min(x,Max))\]

<p>Where \(max(a,b)\) returns the larger of the two values, and \(min(a,b)\) returns the smaller of the two values \(a,b\), or</p>

\[f(x)=max(Min,min(x \times scale + bias,Max))\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisecosoperatordesc">ElementWiseCosOperatorDesc</h4>

<p>Computes the trigonometric cosine of each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing 
the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x)=cos(x)\]

<p>or</p>

\[f(x)=cos(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisedivideoperatordesc">ElementWiseDivideOperatorDesc</h4>

<p>Computes the quotient of each element of <code class="language-plaintext highlighter-rouge">ATensor</code> over the corresponding element 
of <code class="language-plaintext highlighter-rouge">BTensor</code>, placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)=\frac{a}{b}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwiseexpoperatordesc">ElementWiseExpOperatorDesc</h4>

<p>Applies the natural exponentiation function to each element of InputTensor, placing the result into the corresponding element of OutputTensor.</p>

\[f(x)=e^x\]

<p>or</p>

\[f(x)=e^{x \times scale + bias}\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiseflooroperatordesc">ElementWiseFloorOperatorDesc</h4>

<p>Computes the floor for each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

<p>The floor of \(x\) is the largest integer that is less than or equal to \(x\).</p>

\[f(x)=floor(x)\]

<p>or</p>

\[f(x)=floor(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiselogoperatordesc">ElementWiseLogOperatorDesc</h4>

<p>Computes the base-e (natural) logarithm of each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

<p>If \(x\) is negative, then this function returns <code class="language-plaintext highlighter-rouge">NaN</code>. If \(x\) is 0, then this function returns \(-\infty\).</p>

\[f(x)=\ln x\]

<p>or</p>

\[f(x)=\ln{(x \times scale + bias)}\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiselogicalandoperatordesc">ElementWiseLogicalAndOperatorDesc</h4>

<p>Performs a <em>logical and</em> on each pair of corresponding elements of 
the input tensors, placing the result (<code class="language-plaintext highlighter-rouge">1</code> for <code class="language-plaintext highlighter-rouge">true</code>, <code class="language-plaintext highlighter-rouge">0</code> for <code class="language-plaintext highlighter-rouge">false</code>) 
into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= a \land b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwiselogicalequalsoperatordesc">ElementWiseLogicalEqualsOperatorDesc</h4>

<p>Performs a <em>logical equals</em> on each pair of corresponding elements of 
the input tensors, placing the result (<code class="language-plaintext highlighter-rouge">1</code> for <code class="language-plaintext highlighter-rouge">true</code>, <code class="language-plaintext highlighter-rouge">0</code> for <code class="language-plaintext highlighter-rouge">false</code>) 
into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= a \Leftrightarrow b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwiselogicalgreaterthanoperatordesc">ElementWiseLogicalGreaterThanOperatorDesc</h4>

<p>Performs a <em>logical greater than</em> on each pair of corresponding elements of 
the input tensors, placing the result (<code class="language-plaintext highlighter-rouge">1</code> for <code class="language-plaintext highlighter-rouge">true</code>, <code class="language-plaintext highlighter-rouge">0</code> for <code class="language-plaintext highlighter-rouge">false</code>) 
into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= a &gt; b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<h4 id="elementwiselogicallessthanoperatordesc">ElementWiseLogicalLessThanOperatorDesc</h4>

<p>Performs a <em>logical less than</em> on each pair of corresponding elements of 
the input tensors, placing the result (<code class="language-plaintext highlighter-rouge">1</code> for <code class="language-plaintext highlighter-rouge">true</code>, <code class="language-plaintext highlighter-rouge">0</code> for <code class="language-plaintext highlighter-rouge">false</code>) 
into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= a &lt; b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<h4 id="elementwiselogicalnotoperatordesc">ElementWiseLogicalNotOperatorDesc</h4>

<p>Performs a logical NOT on each element of InputTensor, placing the result into the corresponding element of OutputTensor.</p>

\[f(x)= \neg x\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiselogicaloroperatordesc">ElementWiseLogicalOrOperatorDesc</h4>

<p>Performs a logical OR on each pair of corresponding elements of the input 
tensors, placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= a \lor b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwiselogicalxoroperatordesc">ElementWiseLogicalXorOperatorDesc</h4>

<p>Performs a logical XOR (exclusive or) on each pair of corresponding elements 
of the input tensors, placing the result into the corresponding element of 
<code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= a \oplus b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwisemaxoperatordesc">ElementWiseMaxOperatorDesc</h4>

<p>Takes the greater of two corresponding elements from the input tensors, 
and places the result into the corresponding element of the output tensor.</p>

\[f(a,b)= max(a, b)\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwisemeanoperatordesc">ElementWiseMeanOperatorDesc</h4>

<p>Averages each pair of corresponding elements of the input tensors, 
placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= \frac{a+b}{2}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwiseminoperatordesc">ElementWiseMinOperatorDesc</h4>

<p>Takes the lesser of two corresponding elements from the input tensors, 
and places the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= min(a, b)\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwisepowoperatordesc">ElementWisePowOperatorDesc</h4>

<p>Computes each element of <code class="language-plaintext highlighter-rouge">InputTensor</code> raised to the power of the 
corresponding element of <code class="language-plaintext highlighter-rouge">ExponentTensor</code>, placing the result into 
the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

<p>Negative bases are supported for exponents with integral values 
(though datatype can still be float), otherwise this operator returns <code class="language-plaintext highlighter-rouge">NaN</code>.</p>

<p>When the input tensor and exponent tensor both have integral data type, 
this operator guarantees exact results.</p>

\[f(x,y)= x^y\]

<p>or</p>

\[f(x,y)= (x \times scale + bias)^y\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">exponentTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code>, or <code class="language-plaintext highlighter-rouge">ExponentTensor</code>, during binding.</p>

<h4 id="elementwiseconstantpowoperatordesc">ElementWiseConstantPowOperatorDesc</h4>

<p>Raises each element of <code class="language-plaintext highlighter-rouge">InputTensor</code> to the power 
of <code class="language-plaintext highlighter-rouge">Exponent</code>, placing the result into the corresponding 
element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x,y)= x^y\]

<p>or</p>

\[f(x,y)= (x \times scale + bias)^y\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">exponent</code></td>
      <td><code class="language-plaintext highlighter-rouge">float</code></td>
      <td>The exponent that all inputs will be raised to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisereciprocaloperatordesc">ElementWiseReciprocalOperatorDesc</h4>

<p>Computes the reciprocal for each element of the input tensor, placing 
the result into the corresponding element of the output tensor.</p>

\[f(x)= \frac{1}{x}\]

<p>or</p>

\[f(x)= \frac{1}{x \times scale + bias}\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisesinoperatordesc">ElementWiseSinOperatorDesc</h4>

<p>Computes the trigonometric sine of each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, 
placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x)= sin(x)\]

<p>or</p>

\[f(x)= sin(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisesqrtoperatordesc">ElementWiseSqrtOperatorDesc</h4>

<p>Computes the square root of each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, 
placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x)= \sqrt{x}\]

<p>or</p>

\[f(x)= \sqrt{x \times scale + bias}\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisesubtractoperatordesc">ElementWiseSubtractOperatorDesc</h4>

<p>Subtracts each element of <code class="language-plaintext highlighter-rouge">BTensor</code> from the corresponding element 
of <code class="language-plaintext highlighter-rouge">ATensor</code>, placing the result into the corresponding element of 
<code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)=a-b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, or both, during binding.</p>

<h4 id="elementwisetanoperatordesc">ElementWiseTanOperatorDesc</h4>

<p>Computes the trigonometric tangent of each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing 
the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x)= tan(x)\]

<p>or</p>

\[f(x)= tan(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisethresholdoperatordesc">ElementWiseThresholdOperatorDesc</h4>

<p>Replaces all elements of <code class="language-plaintext highlighter-rouge">InputTensor</code> below the given threshold, <code class="language-plaintext highlighter-rouge">Min</code>, with <code class="language-plaintext highlighter-rouge">Min</code>. 
Results are placed into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x)= max(x,Min)\]

<p>or</p>

\[f(x)= max(x \times scale + bias,Min)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">min</code></td>
      <td><code class="language-plaintext highlighter-rouge">float</code></td>
      <td>The minimum value, below which the operator replaces the value with <code class="language-plaintext highlighter-rouge">Min</code>.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisequantizelinearoperatordesc">ElementWiseQuantizeLinearOperatorDesc</h4>

<p>Performs the following linear quantization function on every element 
in <code class="language-plaintext highlighter-rouge">InputTensor</code> with respect to its corresponding element in <code class="language-plaintext highlighter-rouge">ScaleTensor</code> 
and <code class="language-plaintext highlighter-rouge">ZeroPointTensor</code>, placing the results in the corresponding element 
of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(input, scale, zeroPoint) = clamp(round(\frac{input}{scale}) + zeroPoint, Min, Max)\]

<p>Where Min is 0 and Max is 255 for UInt8 output, and Min is -128 and Max is 127 for Int8 output.</p>

<p>Quantizing involves converting to a lower-precision data type in order 
to accelerate arithmetic. Its a common way to increase performance at 
the cost of precision. A group of 8-bit values can be computed faster 
than a group of 32-bit values can.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>The tensor containing the scales. If InputTensor is Int32, then ScaleTensor must be Float32. Otherwise, ScaleTensor must have the same DataType as InputTensor. A scale value of 0 results in undefined behavior.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">zeroPointTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>The tensor containing the desired zero point for the quantization.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="elementwisedequantizelinearoperatordesc">ElementWiseDequantizeLinearOperatorDesc</h4>

<p>Performs the following linear dequantization function on every 
element in <code class="language-plaintext highlighter-rouge">InputTensor</code> with respect to its corresponding element 
in <code class="language-plaintext highlighter-rouge">ScaleTensor</code> and <code class="language-plaintext highlighter-rouge">ZeroPointTensor</code>, placing the results in the 
corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(input, scale, zeroPoint) = (input - zeroPoint) \times scale\]

<p>Quantization is a common way to increase performance at the cost of precision. 
A group of 8-bit int values can be computed faster than a group of 32-bit 
float values can. Dequantizing converts the encoded data back to its domain.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>The tensor containing the scales. If InputTensor is Int32, then ScaleTensor must be Float32. Otherwise, ScaleTensor must have the same DataType as InputTensor. A scale value of 0 results in undefined behavior.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">zeroPointTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>The tensor containing the zero point that was used for quantization.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="activationeluoperatordesc">ActivationELUOperatorDesc</h4>

<p>Performs an exponential linear unit (ELU) activation function on 
every element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding 
element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = \begin{cases}
        x, &amp; \text{ if } x &gt; 0\\
        \alpha \times (\exp(x) - 1), &amp; \text{ if } x \leq 0
        \end{cases}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float</code></td>
      <td>The alpha coefficient. The default for this value is 1.0.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationhardmaxoperatordesc">ActivationHardMaxOperatorDesc</h4>

<p>Performs a hardmax function on each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, 
placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

<p>The operator computes the hardmax (1 for the first occurrence 
of the largest value in the layer, and 0 for all other values) 
of each row in the given input.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from. This tensor must have an effective rank no greater than 2. The effective rank of a tensor is the DimensionCount of the tensor, excluding leftmost dimensions of size 1. For example a tensor size of [ 1, 1, BatchCount, Width ] is valid, and is equivalent to a tensor of sizes [ BatchCount, Width ].</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>The operator computes the hardmax (1 for the first maximum value, 
and 0 for all others) values for each layer in the batch of the 
given input. The input is a 2-D tensor of size (batch_size x input_feature_dimensions). 
The output tensor has the same shape and contains the hardmax values of the corresponding input.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationhardsigmoidoperatordesc">ActivationHardSigmoidOperatorDesc</h4>

<p>Performs a hard sigmoid function on every element in InputTensor, placing the result into the corresponding element of OutputTensor.</p>

\[f(x) = max(0, min(Alpha \times x + Beta, 1))\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The alpha coefficient. The default for this value is 0.2.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">beta</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The beta coefficient. The default for this value is 0.5.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationidentityoperatordesc">ActivationIdentityOperatorDesc</h4>

<p>Performs the identity activation, effectively copying every 
element of <code class="language-plaintext highlighter-rouge">InputTensor</code> to the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = x\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The alpha coefficient. The default for this value is 0.2.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">beta</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The beta coefficient. The default for this value is 0.5.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationleakyreluoperatordesc">ActivationLeakyReLUOperatorDesc</h4>

<p>Performs a leaky rectified linear unit (ReLU) activation function 
on every element in InputTensor, placing the result into the corresponding 
element of OutputTensor.</p>

\[f(x) = \begin{cases}
        x, &amp; \text{ if } x \geq 0\\
        \alpha \times x, &amp; \text{ if } x &lt; 0
        \end{cases}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float</code></td>
      <td>The alpha coefficient. The default for this value is 0.01.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationhardsigmoidoperatordesc-1">ActivationHardSigmoidOperatorDesc</h4>

<p>Performs the linear activation function on every element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, 
placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) =  Alpha \times x + Beta\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The alpha coefficient. The default for this value is 1.0.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">beta</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The beta coefficient. The default for this value is 0.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationlogsoftmaxoperatordesc">ActivationLogSoftMaxOperatorDesc</h4>

<p>Performs a natural log-of-softmax activation function on 
each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding 
element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from. This tensor must have an effective rank no greater than 2. The effective rank of a tensor is the DimensionCount of the tensor, excluding leftmost dimensions of size 1. For example a tensor size of [ 1, 1, BatchCount, Width ] is valid, and is equivalent to a tensor of sizes [ BatchCount, Width ].</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<h4 id="activationparameterizedreluoperatordesc">ActivationParameterizedReLUOperatorDesc</h4>

<p>Performs a parameterized rectified linear unit (ReLU) activation 
function on every element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into 
the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x, slope) = \begin{cases}
        x, &amp; \text{ if } x \geq 0\\
        slope \times x, &amp; \text{ if } x &lt; 0
        \end{cases}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">slopeTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>A tensor containing the slope for each corresponding value of the input.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="activationparametricsoftplusoperatordesc">ActivationParametricSoftPlusOperatorDesc</h4>

<p>Performs a parametric softplus activation function on every element 
in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding element 
of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) =  Alpha \times \log(1 + e^{Beta * x})\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The alpha coefficient.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">beta</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The beta coefficient.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationreluoperatordesc">ActivationReLUOperatorDesc</h4>

<p>Performs a rectified linear unit (ReLU) activation function on every 
element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding 
element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) =  max(0,x)\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationscaledeluoperatordesc">ActivationScaledELUOperatorDesc</h4>

<p>Performs a scaled exponential linear unit (ELU) activation function on 
every element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding 
element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = \begin{cases}
        Gamma \times x, &amp; \text{ if } x &gt; 0\\
        Gamma \times (Alpha \times e^x - Alpha), &amp; \text{ if } x \leq 0
        \end{cases}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The alpha coefficient. The default value is 1.67326319217681884765625.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">gamma</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The beta coefficient. The default value is 1.05070102214813232421875.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationscaledtanhoperatordesc">ActivationScaledTanHOperatorDesc</h4>

<p>Performs a scaled hyperbolic tangent activation function on every 
element in InputTensor, placing the result into the corresponding 
element of OutputTensor.</p>

\[f(x) = Alpha \times tanh(Beta \times x)\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The alpha coefficient. The default value is 1.0</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">beta</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The beta coefficient. The default value is 0.5</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationsigmoidoperatordesc">ActivationSigmoidOperatorDesc</h4>

<p>Performs the sigmoid function on every element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing 
the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = \frac{1}{1 + e^{-x}}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationsoftmaxoperatordesc">ActivationSoftMaxOperatorDesc</h4>

<p>Performs a softmax activation function on InputTensor, placing the 
result into the corresponding element of OutputTensor.</p>

\[f(x_{i}) = \frac{e^{x_i}}{\sum_j e^{x_j}}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from. This tensor must have an effective rank no greater than 2. The effective rank of a tensor is the DimensionCount of the tensor, excluding leftmost dimensions of size 1. For example a tensor size of [ 1, 1, BatchCount, Width ] is valid, and is equivalent to a tensor of sizes [ BatchCount, Width ].</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<h4 id="activationsoftplusoperatordesc">ActivationSoftPlusOperatorDesc</h4>

<p>Performs a parametric softplus activation function on every 
element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding 
element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = \frac{\log(1 + e^{Steepness \times x})}{ Steepness}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">steepness</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The Steepness coefficient. The default value is 1.0. This value cannot be less than 1.0.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationsoftsignoperatordesc">ActivationSoftSignOperatorDesc</h4>

<p>Performs the softsign function on every element in 
<code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding 
element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = \frac{x}{1+|x|}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationtanhoperatordesc">ActivationTanHOperatorDesc</h4>

<p>Performs a hyperbolic tangent activation function on 
every element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into 
the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = \frac{1 - e^{-2 \times x}}{1 + e^{-2 \times x}}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationthresholdedreluoperatordesc">ActivationThresholdedReLUOperatorDesc</h4>

<p>Performs a thresholded rectified linear unit (ReLU) activation 
function on every element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into 
the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = \begin{cases}
        x, &amp; \text{ if } x &gt; \alpha\\
        0, &amp; \text{ if } x \leq \alpha
        \end{cases}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The alpha coefficient. The default value is 1.0</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="convolutionoperatordesc">ConvolutionOperatorDesc</h4>

<p>Performs a convolution of the FilterTensor with the InputTensor. This 
operator supports a number of standard convolution configurations. 
These standard configurations include forward and backward (transposed) 
convolution by setting the Direction and Mode fields, as well as 
depth-wise convolution by setting the GroupCount field.</p>

<p>A summary of the steps involved:</p>

<ol>
  <li>Perform the convolution into the output tensor.</li>
  <li>Reshape the bias to the same dimension sizes as the output tensor.</li>
  <li>Add the reshaped bias tensor to the output tensor.</li>
</ol>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">filterTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>A tensor containing the filter data.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">biasTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>An optional tensor containing the bias data. The bias tensor is a tensor containing data which is broadcasted across the output tensor at the end of the convolution which is added to the result.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">mode</code></td>
      <td><code class="language-plaintext highlighter-rouge">ConvolutionMode</code></td>
      <td>The mode to use for the convolution operation. The default value is <code class="language-plaintext highlighter-rouge">ConvolutionMode::CrossCorrelation</code>.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">direction</code></td>
      <td><code class="language-plaintext highlighter-rouge">ConvolutionDirection</code></td>
      <td>The direction of the convolution operation. The default value is <code class="language-plaintext highlighter-rouge">ConvolutionDirection::Forward</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dimensionCount</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The number of spatial dimensions for the convolution operation. Spatial dimensions are the lower dimensions of the convolution <code class="language-plaintext highlighter-rouge">FilterTensor</code>. For example, the width and height dimension are spatial dimensions of a 4D convolution filter tensor. This value also determines the size of the <code class="language-plaintext highlighter-rouge">Strides</code>, <code class="language-plaintext highlighter-rouge">Dilations</code>, <code class="language-plaintext highlighter-rouge">StartPadding</code>, <code class="language-plaintext highlighter-rouge">EndPadding</code>, and <code class="language-plaintext highlighter-rouge">OutputPadding</code> arrays. It should be set to 2 when <code class="language-plaintext highlighter-rouge">InputTensor.DimensionCount</code> is 4, and 3 when <code class="language-plaintext highlighter-rouge">InputTensor.DimensionCount</code> is 5.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">strides</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>An array containing the strides of the convolution operation. These strides are applied to the convolution filter.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dilations</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>An array containing the dilations of the convolution operation. Dilations are strides applied to the elements of the filter kernel. This has the effect of simulating a larger filter kernel by padding the internal filter kernel elements with zeros.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">startPadding</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>An array containing the padding values to be applied to the beginning of each spatial dimension of the filter and input tensor of the convolution operation. The start padding values are interpreted according to the <code class="language-plaintext highlighter-rouge">Direction</code> field.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">endPadding</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>An array containing the padding values to be applied to the end of each spatial dimension of the filter and input tensor of the convolution operation. The end padding values are interpreted according to the Direction field.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputPadding</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>An array containing the output padding of the convolution operation. OutputPadding applies a zero padding to the result of the convolution. This padding is applied to the end of each spatial dimension of the output tensor.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">groupCount</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The number of groups which to divide the convolution operation up into. This can be used to achieve depth-wise convolution by setting GroupCount equal to the input channel count, and <code class="language-plaintext highlighter-rouge">Direction</code> equal to <code class="language-plaintext highlighter-rouge">ConvolutionDirection::Forward</code>. This divides the convolution up into a separate convolution per input channel.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">fusedActivation</code></td>
      <td><code class="language-plaintext highlighter-rouge">const OperatorDesc*</code></td>
      <td>An optional fused activation layer to apply after the convolution.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>


  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Harlinn</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Harlinn</li><li><a class="u-email" href="mailto:espen@harlinn.no">espen@harlinn.no</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/Harlinn"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">Harlinn</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is my personal site. </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
