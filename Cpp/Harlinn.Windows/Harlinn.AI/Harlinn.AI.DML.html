<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Harlinn.AI DirectML (DML) | Harlinn</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Harlinn.AI DirectML (DML)" />
<meta name="author" content="Espen Harlinn" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is my personal site." />
<meta property="og:description" content="This is my personal site." />
<link rel="canonical" href="https://harlinn.github.io/Cpp/Harlinn.Windows/Harlinn.AI/Harlinn.AI.DML.html" />
<meta property="og:url" content="https://harlinn.github.io/Cpp/Harlinn.Windows/Harlinn.AI/Harlinn.AI.DML.html" />
<meta property="og:site_name" content="Harlinn" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Harlinn.AI DirectML (DML)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Espen Harlinn"},"description":"This is my personal site.","headline":"Harlinn.AI DirectML (DML)","url":"https://harlinn.github.io/Cpp/Harlinn.Windows/Harlinn.AI/Harlinn.AI.DML.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://harlinn.github.io/feed.xml" title="Harlinn" /><script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="importmap">
        {
            "imports":
            {
            "three": "https://cdn.jsdelivr.net/npm/three@0.171.0/build/three.module.min.js" ,
            "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.171.0/examples/jsm/"
            }
        }
    </script>
</head>
<body><header class="site-header" role="banner">
    <div id="animatedHeader" height="120px" width="100%">
        <script type="x-shader/x-vertex" id="vertexshader">
            attribute float scale;
            void main() {
                vec4 mvPosition = modelViewMatrix * vec4( position, 1.0 );
                gl_PointSize = scale * ( 100.0 / - mvPosition.z );
                gl_Position = projectionMatrix * mvPosition;
            }

        </script>

        <script type="x-shader/x-fragment" id="fragmentshader">
            uniform vec3 color;
            void main() {
                if ( length( gl_PointCoord - vec2( 0.5, 0.5 ) ) > 0.475 )  discard;
                gl_FragColor = vec4( color, 0.8 );
            }
        </script>

        <script type="module">

			import * as THREE from 'three';

			const SEPARATION = 100, AMOUNTX = 50, AMOUNTY = 50;

			let container; 
			let camera, scene, renderer;

			let particles, count = 0;

			let mouseX = 0, mouseY = 0;

			let animatedHeaderHeight = 120;

            let windowHalfX = window.innerWidth / 2;
            let windowHalfY = animatedHeaderHeight/2.0;

			init();

			function init() {

                container = document.querySelector('#animatedHeader');

                camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 1, 10000 );
				camera.position.z = 1000;

				scene = new THREE.Scene();

				//

				const numParticles = AMOUNTX * AMOUNTY;

				const positions = new Float32Array( numParticles * 3 );
				const scales = new Float32Array( numParticles );

				let i = 0, j = 0;

				for ( let ix = 0; ix < AMOUNTX; ix ++ ) {

					for ( let iy = 0; iy < AMOUNTY; iy ++ ) {

						positions[ i ] = ix * SEPARATION - ( ( AMOUNTX * SEPARATION ) / 2 ); // x
						positions[ i + 1 ] = 0; // y
						positions[ i + 2 ] = iy * SEPARATION - ( ( AMOUNTY * SEPARATION ) / 2 ); // z

						scales[ j ] = 1;

						i += 3;
						j ++;

					}

				}

				const geometry = new THREE.BufferGeometry();
				geometry.setAttribute( 'position', new THREE.BufferAttribute( positions, 3 ) );
				geometry.setAttribute( 'scale', new THREE.BufferAttribute( scales, 1 ) );

				const material = new THREE.ShaderMaterial( {

					uniforms: {
						color: { value: new THREE.Color( 0xffffff ) },
					},
					vertexShader: document.getElementById( 'vertexshader' ).textContent,
					fragmentShader: document.getElementById( 'fragmentshader' ).textContent

				} );

				//

				particles = new THREE.Points( geometry, material );
				scene.add( particles );

				//

				renderer = new THREE.WebGLRenderer( { antialias: true } );
				renderer.setPixelRatio( window.devicePixelRatio );
                renderer.setSize(window.innerWidth, animatedHeaderHeight );
				renderer.setAnimationLoop( animate );
				container.appendChild( renderer.domElement );

				container.style.touchAction = 'none';
				container.addEventListener( 'pointermove', onPointerMove );

				//

                window.addEventListener( 'resize', onWindowResize );

			}

			function onWindowResize() {

				windowHalfX = window.innerWidth / 2;

				camera.aspect = window.innerWidth / animatedHeaderHeight; 
				camera.updateProjectionMatrix();

				renderer.setSize(window.innerWidth, animatedHeaderHeight);

			}

			//

			function onPointerMove( event ) {

				if ( event.isPrimary === false ) return;

				mouseX = event.clientX - windowHalfX;
				mouseY = event.clientY - windowHalfY;

			}

			//

			function animate() {
				render();

			}

			function render() {

				camera.position.x += ( mouseX - camera.position.x ) * .05;
				camera.position.y += ( - mouseY - camera.position.y ) * .05;
				camera.lookAt( scene.position );

				const positions = particles.geometry.attributes.position.array;
				const scales = particles.geometry.attributes.scale.array;

				let i = 0, j = 0;

				for ( let ix = 0; ix < AMOUNTX; ix ++ ) {

					for ( let iy = 0; iy < AMOUNTY; iy ++ ) {

						positions[ i + 1 ] = ( Math.sin( ( ix + count ) * 0.3 ) * 50 ) +
										( Math.sin( ( iy + count ) * 0.5 ) * 50 );

						scales[ j ] = ( Math.sin( ( ix + count ) * 0.3 ) + 1 ) * 20 +
										( Math.sin( ( iy + count ) * 0.5 ) + 1 ) * 20;

						i += 3;
						j ++;

					}

				}

				particles.geometry.attributes.position.needsUpdate = true;
				particles.geometry.attributes.scale.needsUpdate = true;

				renderer.render( scene, camera );

				count += 0.1;

			}

        </script>

    </div>
  <div class="wrapper"><a class="site-title" rel="author" href="/">Harlinn</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Cpp/Cpp.html">C++</a><a class="page-link" href="/Databases/Databases.html">Databases</a><a class="page-link" href="/DotNet/DotNet.html">.Net</a><a class="page-link" href="/Java/Java.html">Java</a><a class="page-link" href="/Julia/Julia.html">Julia</a><a class="page-link" href="/Python/Python.html">Python</a><a class="page-link" href="/R/R.html">R</a><a class="page-link" href="/Rust/Rust.html">Rust</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Harlinn.AI DirectML (DML)</h1>
  </header>

  <div class="post-content">
    <p>The DirectML wrappers are a key component of Harlinn.AI, providing an API on top 
of the DirectML API, facilitating robust C++ DirectML development.</p>

<p><img src="/Cpp/Harlinn.Windows/Examples/AI\DirectML/Yolo9/ObjectDetection3.png" alt="Yolo v9" /></p>

<p>DirectML is a high-performance, hardware-accelerated DirectX 12 library for 
executing AI workloads. DirectML provides hardware acceleration for common AI 
tasks across a broad range of hardware, including neural processing units (NPU) 
and DirectX 12-capable GPUs.</p>

<p>NPUs are processing units that simulates the neural network of the brain, capable of
processing large amounts of data in parallel, performing trillions of operations per 
second, providing a cost efficient alternative for AI workloads.</p>

<p>Using DirectML we can create hardware accelerated AI apps capable of leveraging 
the capabilities of the system for executing those workloads efficiently, regardless of whether
they run on GPUs or NPUs.</p>

<p>DirectML leverages the Direct3D 12 execution model, where efficient execution of AI
workloads can be interleaved with execution of graphic workloads. For now, this
is an important feature of DirectML, as the current generation of NPUs lacks the
processing power of high end GPUs.</p>

<p>DirectML executes hardware-accelerated AI primitives, called operators, on a suitable 
device on the system running the AI workload. Operators are building blocks that 
can be executed individually, or composed into a graph that fully describes
an AI task that can be compiled and executed by DirectML.</p>

<p>Harlinn.AI redeclares the description operator structures provided with DirectML,
providing a complete set of new structures describing all the operators implemented
by DirectML. Every operator describing structure in the <code class="language-plaintext highlighter-rouge">Harlinn::AI::DML</code> namespace
is binary compatible with the original structure from DirectML describing a particular
operator, but with some key differences:</p>

<ol>
  <li>A <code class="language-plaintext highlighter-rouge">static constexpr DML::OperatorType OperatorType</code> identifying the operator.</li>
  <li>All operator describing structures are directly, or indirectly, derived from <code class="language-plaintext highlighter-rouge">struct BaseOperatorDesc</code>.</li>
  <li>Every operator member variable gets initialized to a default value.</li>
  <li>The operator describing structures have constructors accepting similar arguments, in the same order, even when
the struct declares its members in a different order.</li>
  <li>Nearly all unary operators are derived from <code class="language-plaintext highlighter-rouge">struct UnaryOperatorDesc</code>. The exceptions are the
unary operators defined by DirectML that have an incompatible memory layout.</li>
  <li>Nearly all binary operators are derived from <code class="language-plaintext highlighter-rouge">struct BinaryOperatorDesc</code>. The exceptions are the
binary operators defined by DirectML that have an incompatible memory layout.</li>
</ol>

<p>These features makes it easier to create template based code in C++ for the operators.</p>

<p>This is used by <a href="Harlinn.AI.DML.X.html">Harlinn.AI DirectML eXtensions (DML.X)</a> to implement
a framework that makes it easier to compose a graph of operators that describes an AI task
that can be compiled and executed by DirectML.</p>

<p>The DirectML wrappers can also be used with the <a href="https://www.nuget.org/packages/Microsoft.AI.MachineLearning/">Microsoft.AI.MachineLearning</a> package,
which is a cross-platform library that supports the open standard ONNX format 
for machine learning models. The ONNX Runtime can use DirectML as one of its 
execution providers, along with other backends such as CPU, CUDA, or TensorRT.</p>

<h3 id="operator-descriptors">Operator Descriptors</h3>

<p>Operator descriptors are used to define the datatype and shape of input and output
tensors. In many cases the operator descriptors also hold additional parameters 
for the operation they describe.</p>

<h4 id="baseoperatordesc">BaseOperatorDesc</h4>

<p>All operator descriptors are derived from <code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code>, an empty struct
adding zero bytes to the derived operator descriptors due to <a href="https://en.cppreference.com/w/cpp/language/ebo">Empty base optimization</a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>struct BaseOperatorDesc abstract
{
};
</code></pre></div></div>

<h4 id="unaryoperatordesc">UnaryOperatorDesc</h4>

<p>Nearly all unary DirectML operators have <code class="language-plaintext highlighter-rouge">const TensorDesc* InputTensor</code> and <code class="language-plaintext highlighter-rouge">const TensorDesc* OutputTensor</code> as
the first two member fields of their descriptor type. When this is the case, Harlinn.AI DirectML
derives the operator descriptor from <code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code>.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="unaryoperatorwithscalebiasdesc">UnaryOperatorWithScaleBiasDesc</h4>

<p>Many unary DirectML operators have an optional <code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code> as the third member
field of their descriptor type. When this is the case, Harlinn.AI DirectML
derives the operator descriptor from <code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code>.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<h4 id="binaryoperatordesc">BinaryOperatorDesc</h4>

<p>Many binary DirectML operators have <code class="language-plaintext highlighter-rouge">const TensorDesc* ATensor</code>, <code class="language-plaintext highlighter-rouge">const TensorDesc* BTensor</code> and <code class="language-plaintext highlighter-rouge">const TensorDesc* OutputTensor</code> as
the first three member fields of their descriptor type. When this is the case, Harlinn.AI DirectML
derives the operator descriptor from <code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code>.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="elementwiseidentityoperatordesc">ElementWiseIdentityOperatorDesc</h4>

<p>Computes the identity for each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing 
the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x)=x\]

<p>or</p>

\[f(x)=x \times scale + bias\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiseabsoperatordesc">ElementWiseAbsOperatorDesc</h4>

<p>Computes the absolute value for each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing 
the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x)=|x|\]

<p>or</p>

\[f(x)=|x \times scale + bias|\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiseacosoperatordesc">ElementWiseACosOperatorDesc</h4>

<p>Computes the inverse cosine of each element of InputTensor, placing 
the result into the corresponding element of OutputTensor.</p>

\[f(x)=cos^{-1}(x)\]

<p>or</p>

\[f(x)=cos^{-1}(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiseaddoperatordesc">ElementWiseAddOperatorDesc</h4>

<p>Adds every element in <code class="language-plaintext highlighter-rouge">ATensor</code> to its corresponding element in <code class="language-plaintext highlighter-rouge">BTensor</code>, placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)=a+b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, or both, during binding.</p>

<h4 id="elementwiseadd1operatordesc">ElementWiseAdd1OperatorDesc</h4>

<p>Adds every element in ATensor to its corresponding element in BTensor and places the result 
into the corresponding element of OutputTensor, with the option for fused activation.</p>

\[f(a,b)=a+b\]

<p>or</p>

\[f(a,b)=FusedActivation(a+b)\]

<p>when <code class="language-plaintext highlighter-rouge">fusedActivation</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">fusedActivation</code></td>
      <td><code class="language-plaintext highlighter-rouge">const OperatorDesc*</code></td>
      <td>An optional operator descriptor for a fused activation operator to be applied to the output.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, or both, during binding.</p>

<h4 id="elementwiseasinoperatordesc">ElementWiseASinOperatorDesc</h4>

<p>Computes the arcsine for each element of InputTensor, placing the result 
into the corresponding element of OutputTensor.</p>

\[f(x)=sin^{-1}(x)\]

<p>or</p>

\[f(x)=sin^{-1}(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiseatanoperatordesc">ElementWiseATanOperatorDesc</h4>

<p>Computes the arctangent for each element of InputTensor, 
placing the result into the corresponding element of OutputTensor.</p>

\[f(x)=tan^{-1}(x)\]

<p>or</p>

\[f(x)=tan^{-1}(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiseceiloperatordesc">ElementWiseCeilOperatorDesc</h4>

<p>Computes the ceiling for each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing 
the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>. 
The ceiling of \(x\) is the smallest integer that is greater than or equal to \(x\).</p>

\[f(x)=ceil(x)\]

<p>or</p>

\[f(x)=ceil(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiseclipoperatordesc">ElementWiseClipOperatorDesc</h4>

<p>Performs the following operation for each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing 
the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>. This operator 
clamps (or limits) every element in the input within the closed interval \([Min, Max]\).</p>

\[f(x)=max(Min,min(x,Max))\]

<p>Where \(max(a,b)\) returns the larger of the two values, and \(min(a,b)\) returns the smaller of the two values \(a,b\), or</p>

\[f(x)=max(Min,min(x \times scale + bias,Max))\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisecosoperatordesc">ElementWiseCosOperatorDesc</h4>

<p>Computes the trigonometric cosine of each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing 
the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x)=cos(x)\]

<p>or</p>

\[f(x)=cos(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisedivideoperatordesc">ElementWiseDivideOperatorDesc</h4>

<p>Computes the quotient of each element of <code class="language-plaintext highlighter-rouge">ATensor</code> over the corresponding element 
of <code class="language-plaintext highlighter-rouge">BTensor</code>, placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)=\frac{a}{b}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwiseexpoperatordesc">ElementWiseExpOperatorDesc</h4>

<p>Applies the natural exponentiation function to each element of InputTensor, placing the result into the corresponding element of OutputTensor.</p>

\[f(x)=e^x\]

<p>or</p>

\[f(x)=e^{x \times scale + bias}\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiseflooroperatordesc">ElementWiseFloorOperatorDesc</h4>

<p>Computes the floor for each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

<p>The floor of \(x\) is the largest integer that is less than or equal to \(x\).</p>

\[f(x)=floor(x)\]

<p>or</p>

\[f(x)=floor(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiselogoperatordesc">ElementWiseLogOperatorDesc</h4>

<p>Computes the base-e (natural) logarithm of each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

<p>If \(x\) is negative, then this function returns <code class="language-plaintext highlighter-rouge">NaN</code>. If \(x\) is 0, then this function returns \(-\infty\).</p>

\[f(x)=\ln x\]

<p>or</p>

\[f(x)=\ln{(x \times scale + bias)}\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiselogicalandoperatordesc">ElementWiseLogicalAndOperatorDesc</h4>

<p>Performs a <em>logical and</em> on each pair of corresponding elements of 
the input tensors, placing the result (<code class="language-plaintext highlighter-rouge">1</code> for <code class="language-plaintext highlighter-rouge">true</code>, <code class="language-plaintext highlighter-rouge">0</code> for <code class="language-plaintext highlighter-rouge">false</code>) 
into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= a \land b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwiselogicalequalsoperatordesc">ElementWiseLogicalEqualsOperatorDesc</h4>

<p>Performs a <em>logical equals</em> on each pair of corresponding elements of 
the input tensors, placing the result (<code class="language-plaintext highlighter-rouge">1</code> for <code class="language-plaintext highlighter-rouge">true</code>, <code class="language-plaintext highlighter-rouge">0</code> for <code class="language-plaintext highlighter-rouge">false</code>) 
into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= a \Leftrightarrow b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwiselogicalgreaterthanoperatordesc">ElementWiseLogicalGreaterThanOperatorDesc</h4>

<p>Performs a <em>logical greater than</em> on each pair of corresponding elements of 
the input tensors, placing the result (<code class="language-plaintext highlighter-rouge">1</code> for <code class="language-plaintext highlighter-rouge">true</code>, <code class="language-plaintext highlighter-rouge">0</code> for <code class="language-plaintext highlighter-rouge">false</code>) 
into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= a &gt; b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<h4 id="elementwiselogicallessthanoperatordesc">ElementWiseLogicalLessThanOperatorDesc</h4>

<p>Performs a <em>logical less than</em> on each pair of corresponding elements of 
the input tensors, placing the result (<code class="language-plaintext highlighter-rouge">1</code> for <code class="language-plaintext highlighter-rouge">true</code>, <code class="language-plaintext highlighter-rouge">0</code> for <code class="language-plaintext highlighter-rouge">false</code>) 
into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= a &lt; b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<h4 id="elementwiselogicalnotoperatordesc">ElementWiseLogicalNotOperatorDesc</h4>

<p>Performs a logical NOT on each element of InputTensor, placing the result into the corresponding element of OutputTensor.</p>

\[f(x)= \neg x\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwiselogicaloroperatordesc">ElementWiseLogicalOrOperatorDesc</h4>

<p>Performs a logical OR on each pair of corresponding elements of the input 
tensors, placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= a \lor b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwiselogicalxoroperatordesc">ElementWiseLogicalXorOperatorDesc</h4>

<p>Performs a logical XOR (exclusive or) on each pair of corresponding elements 
of the input tensors, placing the result into the corresponding element of 
<code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= a \oplus b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwisemaxoperatordesc">ElementWiseMaxOperatorDesc</h4>

<p>Takes the greater of two corresponding elements from the input tensors, 
and places the result into the corresponding element of the output tensor.</p>

\[f(a,b)= max(a, b)\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwisemeanoperatordesc">ElementWiseMeanOperatorDesc</h4>

<p>Averages each pair of corresponding elements of the input tensors, 
placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= \frac{a+b}{2}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwiseminoperatordesc">ElementWiseMinOperatorDesc</h4>

<p>Takes the lesser of two corresponding elements from the input tensors, 
and places the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)= min(a, b)\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, during binding.</p>

<h4 id="elementwisepowoperatordesc">ElementWisePowOperatorDesc</h4>

<p>Computes each element of <code class="language-plaintext highlighter-rouge">InputTensor</code> raised to the power of the 
corresponding element of <code class="language-plaintext highlighter-rouge">ExponentTensor</code>, placing the result into 
the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

<p>Negative bases are supported for exponents with integral values 
(though datatype can still be float), otherwise this operator returns <code class="language-plaintext highlighter-rouge">NaN</code>.</p>

<p>When the input tensor and exponent tensor both have integral data type, 
this operator guarantees exact results.</p>

\[f(x,y)= x^y\]

<p>or</p>

\[f(x,y)= (x \times scale + bias)^y\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">exponentTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code>, or <code class="language-plaintext highlighter-rouge">ExponentTensor</code>, during binding.</p>

<h4 id="elementwiseconstantpowoperatordesc">ElementWiseConstantPowOperatorDesc</h4>

<p>Raises each element of <code class="language-plaintext highlighter-rouge">InputTensor</code> to the power 
of <code class="language-plaintext highlighter-rouge">Exponent</code>, placing the result into the corresponding 
element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x,y)= x^y\]

<p>or</p>

\[f(x,y)= (x \times scale + bias)^y\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">exponent</code></td>
      <td><code class="language-plaintext highlighter-rouge">float</code></td>
      <td>The exponent that all inputs will be raised to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisereciprocaloperatordesc">ElementWiseReciprocalOperatorDesc</h4>

<p>Computes the reciprocal for each element of the input tensor, placing 
the result into the corresponding element of the output tensor.</p>

\[f(x)= \frac{1}{x}\]

<p>or</p>

\[f(x)= \frac{1}{x \times scale + bias}\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisesinoperatordesc">ElementWiseSinOperatorDesc</h4>

<p>Computes the trigonometric sine of each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, 
placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x)= sin(x)\]

<p>or</p>

\[f(x)= sin(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisesqrtoperatordesc">ElementWiseSqrtOperatorDesc</h4>

<p>Computes the square root of each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, 
placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x)= \sqrt{x}\]

<p>or</p>

\[f(x)= \sqrt{x \times scale + bias}\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisesubtractoperatordesc">ElementWiseSubtractOperatorDesc</h4>

<p>Subtracts each element of <code class="language-plaintext highlighter-rouge">BTensor</code> from the corresponding element 
of <code class="language-plaintext highlighter-rouge">ATensor</code>, placing the result into the corresponding element of 
<code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(a,b)=a-b\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorA</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the first input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorB</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the second input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#binaryoperatordesc"><code class="language-plaintext highlighter-rouge">BinaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to 
the same tensor as <code class="language-plaintext highlighter-rouge">ATensor</code>, or <code class="language-plaintext highlighter-rouge">BTensor</code>, or both, during binding.</p>

<h4 id="elementwisetanoperatordesc">ElementWiseTanOperatorDesc</h4>

<p>Computes the trigonometric tangent of each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing 
the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x)= tan(x)\]

<p>or</p>

\[f(x)= tan(x \times scale + bias)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisethresholdoperatordesc">ElementWiseThresholdOperatorDesc</h4>

<p>Replaces all elements of <code class="language-plaintext highlighter-rouge">InputTensor</code> below the given threshold, <code class="language-plaintext highlighter-rouge">Min</code>, with <code class="language-plaintext highlighter-rouge">Min</code>. 
Results are placed into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x)= max(x,Min)\]

<p>or</p>

\[f(x)= max(x \times scale + bias,Min)\]

<p>when <code class="language-plaintext highlighter-rouge">scaleBias</code> is specified.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleBias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const DML::ScaleBias*</code></td>
      <td>An optional scale and bias to apply to the input. If present, this has the effect of applying the function \(g(x) = x \times scale + bias\) to each input element prior to computing this operator.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">min</code></td>
      <td><code class="language-plaintext highlighter-rouge">float</code></td>
      <td>The minimum value, below which the operator replaces the value with <code class="language-plaintext highlighter-rouge">Min</code>.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatorwithscalebiasdesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorWithScaleBiasDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="elementwisequantizelinearoperatordesc">ElementWiseQuantizeLinearOperatorDesc</h4>

<p>Performs the following linear quantization function on every element 
in <code class="language-plaintext highlighter-rouge">InputTensor</code> with respect to its corresponding element in <code class="language-plaintext highlighter-rouge">ScaleTensor</code> 
and <code class="language-plaintext highlighter-rouge">ZeroPointTensor</code>, placing the results in the corresponding element 
of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(input, scale, zeroPoint) = clamp(round(\frac{input}{scale}) + zeroPoint, Min, Max)\]

<p>Where Min is 0 and Max is 255 for UInt8 output, and Min is -128 and Max is 127 for Int8 output.</p>

<p>Quantizing involves converting to a lower-precision data type in order 
to accelerate arithmetic. It’s a common way to increase performance at 
the cost of precision. A group of 8-bit values can be computed faster 
than a group of 32-bit values can.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>The tensor containing the scales. If InputTensor is Int32, then ScaleTensor must be Float32. Otherwise, ScaleTensor must have the same DataType as InputTensor. A scale value of 0 results in undefined behavior.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">zeroPointTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>The tensor containing the desired zero point for the quantization.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="elementwisedequantizelinearoperatordesc">ElementWiseDequantizeLinearOperatorDesc</h4>

<p>Performs the following linear dequantization function on every 
element in <code class="language-plaintext highlighter-rouge">InputTensor</code> with respect to its corresponding element 
in <code class="language-plaintext highlighter-rouge">ScaleTensor</code> and <code class="language-plaintext highlighter-rouge">ZeroPointTensor</code>, placing the results in the 
corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(input, scale, zeroPoint) = (input - zeroPoint) \times scale\]

<p>Quantization is a common way to increase performance at the cost of precision. 
A group of 8-bit int values can be computed faster than a group of 32-bit 
float values can. Dequantizing converts the encoded data back to its domain.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>The tensor containing the scales. If InputTensor is Int32, then ScaleTensor must be Float32. Otherwise, ScaleTensor must have the same DataType as InputTensor. A scale value of 0 results in undefined behavior.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">zeroPointTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>The tensor containing the zero point that was used for quantization.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="activationeluoperatordesc">ActivationELUOperatorDesc</h4>

<p>Performs an exponential linear unit (ELU) activation function on 
every element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding 
element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = \begin{cases}
        x, &amp; \text{ if } x &gt; 0\\
        \alpha \times (\exp(x) - 1), &amp; \text{ if } x \leq 0
        \end{cases}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float</code></td>
      <td>The alpha coefficient. The default for this value is 1.0.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationhardmaxoperatordesc">ActivationHardMaxOperatorDesc</h4>

<p>Performs a hardmax function on each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, 
placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

<p>The operator computes the hardmax (1 for the first occurrence 
of the largest value in the layer, and 0 for all other values) 
of each row in the given input.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from. This tensor must have an effective rank no greater than 2. The effective rank of a tensor is the DimensionCount of the tensor, excluding leftmost dimensions of size 1. For example a tensor size of [ 1, 1, BatchCount, Width ] is valid, and is equivalent to a tensor of sizes [ BatchCount, Width ].</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>The operator computes the hardmax (1 for the first maximum value, 
and 0 for all others) values for each layer in the batch of the 
given input. The input is a 2-D tensor of size (batch_size x input_feature_dimensions). 
The output tensor has the same shape and contains the hardmax values of the corresponding input.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationhardsigmoidoperatordesc">ActivationHardSigmoidOperatorDesc</h4>

<p>Performs a hard sigmoid function on every element in InputTensor, placing the result into the corresponding element of OutputTensor.</p>

\[f(x) = max(0, min(Alpha \times x + Beta, 1))\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The alpha coefficient. The default for this value is 0.2.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">beta</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The beta coefficient. The default for this value is 0.5.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationidentityoperatordesc">ActivationIdentityOperatorDesc</h4>

<p>Performs the identity activation, effectively copying every 
element of <code class="language-plaintext highlighter-rouge">InputTensor</code> to the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = x\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The alpha coefficient. The default for this value is 0.2.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">beta</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The beta coefficient. The default for this value is 0.5.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationleakyreluoperatordesc">ActivationLeakyReLUOperatorDesc</h4>

<p>Performs a leaky rectified linear unit (ReLU) activation function 
on every element in InputTensor, placing the result into the corresponding 
element of OutputTensor.</p>

\[f(x) = \begin{cases}
        x, &amp; \text{ if } x \geq 0\\
        \alpha \times x, &amp; \text{ if } x &lt; 0
        \end{cases}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float</code></td>
      <td>The alpha coefficient. The default for this value is 0.01.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationhardsigmoidoperatordesc-1">ActivationHardSigmoidOperatorDesc</h4>

<p>Performs the linear activation function on every element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, 
placing the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) =  Alpha \times x + Beta\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The alpha coefficient. The default for this value is 1.0.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">beta</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The beta coefficient. The default for this value is 0.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationlogsoftmaxoperatordesc">ActivationLogSoftMaxOperatorDesc</h4>

<p>Performs a natural log-of-softmax activation function on 
each element of <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding 
element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from. This tensor must have an effective rank no greater than 2. The effective rank of a tensor is the DimensionCount of the tensor, excluding leftmost dimensions of size 1. For example a tensor size of [ 1, 1, BatchCount, Width ] is valid, and is equivalent to a tensor of sizes [ BatchCount, Width ].</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<h4 id="activationparameterizedreluoperatordesc">ActivationParameterizedReLUOperatorDesc</h4>

<p>Performs a parameterized rectified linear unit (ReLU) activation 
function on every element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into 
the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x, slope) = \begin{cases}
        x, &amp; \text{ if } x \geq 0\\
        slope \times x, &amp; \text{ if } x &lt; 0
        \end{cases}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">slopeTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>A tensor containing the slope for each corresponding value of the input.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="activationparametricsoftplusoperatordesc">ActivationParametricSoftPlusOperatorDesc</h4>

<p>Performs a parametric softplus activation function on every element 
in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding element 
of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) =  Alpha \times \log(1 + e^{Beta * x})\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The alpha coefficient.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">beta</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The beta coefficient.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationreluoperatordesc">ActivationReLUOperatorDesc</h4>

<p>Performs a rectified linear unit (ReLU) activation function on every 
element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding 
element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) =  max(0,x)\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationscaledeluoperatordesc">ActivationScaledELUOperatorDesc</h4>

<p>Performs a scaled exponential linear unit (ELU) activation function on 
every element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding 
element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = \begin{cases}
        Gamma \times x, &amp; \text{ if } x &gt; 0\\
        Gamma \times (Alpha \times e^x - Alpha), &amp; \text{ if } x \leq 0
        \end{cases}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The alpha coefficient. The default value is 1.67326319217681884765625.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">gamma</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The beta coefficient. The default value is 1.05070102214813232421875.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationscaledtanhoperatordesc">ActivationScaledTanHOperatorDesc</h4>

<p>Performs a scaled hyperbolic tangent activation function on every 
element in InputTensor, placing the result into the corresponding 
element of OutputTensor.</p>

\[f(x) = Alpha \times tanh(Beta \times x)\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The alpha coefficient. The default value is 1.0</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">beta</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The beta coefficient. The default value is 0.5</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationsigmoidoperatordesc">ActivationSigmoidOperatorDesc</h4>

<p>Performs the sigmoid function on every element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing 
the result into the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = \frac{1}{1 + e^{-x}}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationsoftmaxoperatordesc">ActivationSoftMaxOperatorDesc</h4>

<p>Performs a softmax activation function on InputTensor, placing the 
result into the corresponding element of OutputTensor.</p>

\[f(x_{i}) = \frac{e^{x_i}}{\sum_j e^{x_j}}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from. This tensor must have an effective rank no greater than 2. The effective rank of a tensor is the DimensionCount of the tensor, excluding leftmost dimensions of size 1. For example a tensor size of [ 1, 1, BatchCount, Width ] is valid, and is equivalent to a tensor of sizes [ BatchCount, Width ].</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<h4 id="activationsoftplusoperatordesc">ActivationSoftPlusOperatorDesc</h4>

<p>Performs a parametric softplus activation function on every 
element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding 
element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = \frac{\log(1 + e^{Steepness \times x})}{ Steepness}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">steepness</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The Steepness coefficient. The default value is 1.0. This value cannot be less than 1.0.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationsoftsignoperatordesc">ActivationSoftSignOperatorDesc</h4>

<p>Performs the softsign function on every element in 
<code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into the corresponding 
element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = \frac{x}{1+|x|}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationtanhoperatordesc">ActivationTanHOperatorDesc</h4>

<p>Performs a hyperbolic tangent activation function on 
every element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into 
the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = \frac{1 - e^{-2 \times x}}{1 + e^{-2 \times x}}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="activationthresholdedreluoperatordesc">ActivationThresholdedReLUOperatorDesc</h4>

<p>Performs a thresholded rectified linear unit (ReLU) activation 
function on every element in <code class="language-plaintext highlighter-rouge">InputTensor</code>, placing the result into 
the corresponding element of <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</p>

\[f(x) = \begin{cases}
        x, &amp; \text{ if } x &gt; \alpha\\
        0, &amp; \text{ if } x \leq \alpha
        \end{cases}\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float*</code></td>
      <td>The alpha coefficient. The default value is 1.0</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<p>This operator supports in-place execution, meaning that <code class="language-plaintext highlighter-rouge">OutputTensor</code> can be bound to the same tensor as <code class="language-plaintext highlighter-rouge">InputTensor</code> during binding.</p>

<h4 id="convolutionoperatordesc">ConvolutionOperatorDesc</h4>

<p>Performs a convolution of the FilterTensor with the InputTensor. This 
operator supports a number of standard convolution configurations. 
These standard configurations include forward and backward (transposed) 
convolution by setting the Direction and Mode fields, as well as 
depth-wise convolution by setting the GroupCount field.</p>

<p>A summary of the steps involved:</p>

<ol>
  <li>Perform the convolution into the output tensor.</li>
  <li>Reshape the bias to the same dimension sizes as the output tensor.</li>
  <li>Add the reshaped bias tensor to the output tensor.</li>
</ol>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">filterTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>A tensor containing the filter data.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">biasTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>An optional tensor containing the bias data. The bias tensor is a tensor containing data which is broadcasted across the output tensor at the end of the convolution which is added to the result.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">mode</code></td>
      <td><code class="language-plaintext highlighter-rouge">ConvolutionMode</code></td>
      <td>The mode to use for the convolution operation. The default value is <code class="language-plaintext highlighter-rouge">ConvolutionMode::CrossCorrelation</code>.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">direction</code></td>
      <td><code class="language-plaintext highlighter-rouge">ConvolutionDirection</code></td>
      <td>The direction of the convolution operation. The default value is <code class="language-plaintext highlighter-rouge">ConvolutionDirection::Forward</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dimensionCount</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The number of spatial dimensions for the convolution operation. Spatial dimensions are the lower dimensions of the convolution <code class="language-plaintext highlighter-rouge">FilterTensor</code>. For example, the width and height dimension are spatial dimensions of a 4D convolution filter tensor. This value also determines the size of the <code class="language-plaintext highlighter-rouge">Strides</code>, <code class="language-plaintext highlighter-rouge">Dilations</code>, <code class="language-plaintext highlighter-rouge">StartPadding</code>, <code class="language-plaintext highlighter-rouge">EndPadding</code>, and <code class="language-plaintext highlighter-rouge">OutputPadding</code> arrays. It should be set to 2 when <code class="language-plaintext highlighter-rouge">InputTensor.DimensionCount</code> is 4, and 3 when <code class="language-plaintext highlighter-rouge">InputTensor.DimensionCount</code> is 5.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">strides</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>An array containing the strides of the convolution operation. These strides are applied to the convolution filter.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dilations</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>An array containing the dilations of the convolution operation. Dilations are strides applied to the elements of the filter kernel. This has the effect of simulating a larger filter kernel by padding the internal filter kernel elements with zeros.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">startPadding</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>An array containing the padding values to be applied to the beginning of each spatial dimension of the filter and input tensor of the convolution operation. The start padding values are interpreted according to the <code class="language-plaintext highlighter-rouge">Direction</code> field.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">endPadding</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>An array containing the padding values to be applied to the end of each spatial dimension of the filter and input tensor of the convolution operation. The end padding values are interpreted according to the Direction field.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputPadding</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>An array containing the output padding of the convolution operation. OutputPadding applies a zero padding to the result of the convolution. This padding is applied to the end of each spatial dimension of the output tensor.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">groupCount</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The number of groups which to divide the convolution operation up into. This can be used to achieve depth-wise convolution by setting GroupCount equal to the input channel count, and <code class="language-plaintext highlighter-rouge">Direction</code> equal to <code class="language-plaintext highlighter-rouge">ConvolutionDirection::Forward</code>. This divides the convolution up into a separate convolution per input channel.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">fusedActivation</code></td>
      <td><code class="language-plaintext highlighter-rouge">const OperatorDesc*</code></td>
      <td>An optional fused activation layer to apply after the convolution.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="gemmoperatordesc">GEMMOperatorDesc</h4>

<p>Performs a general matrix multiplication function of the form 
\(f(a,b,c) = FusedActivation(Alpha * TransA(A) x TransB(B) + Beta * C)\), 
where x denotes matrix multiplication, and * denotes multiplication with a scalar.</p>

<p>This operator requires 4D tensors with layout [ BatchCount, ChannelCount, Height, Width ], 
and it will perform BatchCount * ChannelCount number of independent matrix multiplications.</p>

<p>For example, if ATensor has Sizes of [ BatchCount, ChannelCount, M, K ], 
and BTensor has Sizes of [ BatchCount, ChannelCount, K, N ], and OutputTensor 
has Sizes of [ BatchCount, ChannelCount, M, N ], then this operator 
performs BatchCount * ChannelCount independent matrix multiplications 
of dimensions [M,K] x [K,N] = [M,N].</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">aTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>A tensor containing the A matrix. This tensor’s Sizes should be [ BatchCount, ChannelCount, M, K ] if <code class="language-plaintext highlighter-rouge">TransA</code> is <code class="language-plaintext highlighter-rouge">MatrixTransform::None</code>, or [ BatchCount, ChannelCount, K, M ] if <code class="language-plaintext highlighter-rouge">TransA</code> is <code class="language-plaintext highlighter-rouge">MatrixTransform::Transpose</code>.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">bTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>A tensor containing the B matrix. This tensor’s Sizes should be [ BatchCount, ChannelCount, K, N ] if <code class="language-plaintext highlighter-rouge">TransB</code> is <code class="language-plaintext highlighter-rouge">MatrixTransform::None</code>, or [ BatchCount, ChannelCount, N, K ] if <code class="language-plaintext highlighter-rouge">TransB</code> is <code class="language-plaintext highlighter-rouge">MatrixTransform::Transpose</code>.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>The tensor to write the results to. This tensor’s Sizes are [ BatchCount, ChannelCount, M, N ].</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">cTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>An optional tensor containing the C matrix, or nullptr. Values default to 0 when not provided. If provided, this tensor’s Sizes should be [ BatchCount, ChannelCount, M, N ].</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">alpha</code></td>
      <td><code class="language-plaintext highlighter-rouge">float</code></td>
      <td>The value of the scalar multiplier for the product of inputs ATensor and BTensor.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">beta</code></td>
      <td><code class="language-plaintext highlighter-rouge">float</code></td>
      <td>The value of the scalar multiplier for the optional input CTensor. If CTensor is not provided, then this value is ignored.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">transA</code></td>
      <td><code class="language-plaintext highlighter-rouge">MatrixTransform</code></td>
      <td>The transform to be applied to ATensor; either a transpose, or no transform.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">transB</code></td>
      <td><code class="language-plaintext highlighter-rouge">MatrixTransform</code></td>
      <td>The transform to be applied to BTensor; either a transpose, or no transform.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">fusedActivation</code></td>
      <td><code class="language-plaintext highlighter-rouge">const OperatorDesc*</code></td>
      <td>An optional fused activation layer to apply after the GEMM.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="reduceoperatordesc">ReduceOperatorDesc</h4>

<p>Outputs the reduction of elements (sum, product, minimum, 
and so on) within one or more dimensions of the input tensor.</p>

<p>Each output element is the result of applying a reduction 
function on a subset of the input tensor. A reduction function, 
such as sum, maps N input elements to a single output element. 
The input elements involved in each reduction are determined by 
the provided input axes: N is equal to the product of the sizes 
of the reduced axes. If all input axes are specified, then the 
operator performs a reduction on the entire input tensor and 
produces a single output element.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>The tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>The tensor to write the results to. Each output element is the result of a reduction on a subset of elements from the InputTensor.DimensionCount must match InputTensor.DimensionCount (the rank of the input tensor is preserved). Sizes must match InputTensor.Sizes, except for dimensions included in the reduced Axes, which must be size 1.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">function</code></td>
      <td><code class="language-plaintext highlighter-rouge">ReduceFunction</code></td>
      <td>Specifies the reduction function to apply to the input.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">axisCount</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The number of axes to reduce. This field determines the size of the Axes array.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">axes</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>The axes along which to reduce. Values must be in the range [0, <code class="language-plaintext highlighter-rouge">InputTensor.DimensionCount</code> - 1].</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="averagepoolingoperatordesc">AveragePoolingOperatorDesc</h4>

<p>Averages values across the elements within the sliding window over the input tensor.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dimensionCount</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The number of spatial dimensions of the input tensor InputTensor, which also corresponds to the number of dimensions of the sliding window WindowSize. This value also determines the size of the Strides, StartPadding, and EndPadding arrays. It should be set to 2 when InputTensor is 4D, and 3 when it’s a 5D tensor.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">strides</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>The strides for the sliding window dimensions of sizes [ Height, Width ] when the DimensionCount is set to 2, or [ Depth, Height, Width ] when set to 3.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">windowSize</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>The dimensions of the sliding window in [ Height, Width ] when DimensionCount is set to 2, or [ Depth, Height, Width ] when set to 3.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">startPadding</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>The number of padding elements to be applied to the beginning of each spatial dimension of the input tensor InputTensor. The values are in [ Height, Width ] when DimensionCount is set to 2, or [ Depth, Height, Width ] when set to 3.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">endPadding</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>The number of padding elements to be applied to the end of each spatial dimension of the input tensor InputTensor. The values are in [ Height, Width ] when DimensionCount is set to 2, or [ Depth, Height, Width ] when set to 3.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">includePadding</code></td>
      <td><code class="language-plaintext highlighter-rouge">bool</code></td>
      <td>Indicates whether to include the padding elements around the spatial edges when calculating the average value across all elements within the sliding window. When the value is set to FALSE, the padding elements are not counted as part of the divisor value of the averaging calculation.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<h4 id="lppoolingoperatordesc">LPPoolingOperatorDesc</h4>

<p>Computes the Lp-normalized value across the elements within the sliding window over the input tensor.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dimensionCount</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The number of spatial dimensions of the input tensor InputTensor, which also corresponds to the number of dimensions of the sliding window WindowSize. This value also determines the size of the Strides, StartPadding, and EndPadding arrays. It should be set to 2 when InputTensor is 4D, and 3 when it’s a 5D tensor.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">strides</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>The strides for the sliding window dimensions of sizes [ Height, Width ] when the DimensionCount is set to 2, or [ Depth, Height, Width ] when set to 3.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">windowSize</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>The dimensions of the sliding window in [ Height, Width ] when DimensionCount is set to 2, or [ Depth, Height, Width ] when set to 3.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">startPadding</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>The number of padding elements to be applied to the beginning of each spatial dimension of the input tensor InputTensor. The values are in [ Height, Width ] when DimensionCount is set to 2, or [ Depth, Height, Width ] when set to 3.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">endPadding</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>The number of padding elements to be applied to the end of each spatial dimension of the input tensor InputTensor. The values are in [ Height, Width ] when DimensionCount is set to 2, or [ Depth, Height, Width ] when set to 3.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">p</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The value of the P variable in the Lp-normalization function \(Y = (X1^P + X2^P + ... + Xn^P)^{1/P}\), where X1 to Xn representing each of the values within the sliding window. In common use cases, this value is either set to 1 or 2, representing either the L1 or L2 normalization respectively.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<h4 id="maxpoolingoperatordesc">MaxPoolingOperatorDesc</h4>

<p>Computes the maximum value across the elements within the sliding window over the input tensor.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dimensionCount</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The number of spatial dimensions of the input tensor InputTensor, which also corresponds to the number of dimensions of the sliding window WindowSize. This value also determines the size of the Strides, StartPadding, and EndPadding arrays. It should be set to 2 when InputTensor is 4D, and 3 when it’s a 5D tensor.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">strides</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>The strides for the sliding window dimensions of sizes [ Height, Width ] when the DimensionCount is set to 2, or [ Depth, Height, Width ] when set to 3.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">windowSize</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>The dimensions of the sliding window in [ Height, Width ] when DimensionCount is set to 2, or [ Depth, Height, Width ] when set to 3.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">startPadding</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>The number of padding elements to be applied to the beginning of each spatial dimension of the input tensor InputTensor. The values are in [ Height, Width ] when DimensionCount is set to 2, or [ Depth, Height, Width ] when set to 3.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">endPadding</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>The number of padding elements to be applied to the end of each spatial dimension of the input tensor InputTensor. The values are in [ Height, Width ] when DimensionCount is set to 2, or [ Depth, Height, Width ] when set to 3.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<h4 id="roipoolingoperatordesc">ROIPoolingOperatorDesc</h4>

<p>Performs a MaxPool function across the input tensor according to regions of interest.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">roiTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>A tensor containing the regions of interest (ROI) data. The expected dimensions of ROITensor are [ 1, 1, NumROIs, 5 ] and the data for each ROI is [BatchID, x1, y1, x2, y2]. x1, y1, x2, y2 are the inclusive coordinates of the corners of each ROI and \(x2 &gt;= x1, y2 &gt;= y1.\)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">spatialScale</code></td>
      <td><code class="language-plaintext highlighter-rouge">float</code></td>
      <td>Multiplicative spatial scale factor used to translate the ROI coordinates from their input scale to the scale used when pooling.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">pooledSize</code></td>
      <td><code class="language-plaintext highlighter-rouge">Size2D</code></td>
      <td>The ROI pool output size (height, width), which must match the last 2 dimensions of OutputTensor.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="sliceoperatordesc">SliceOperatorDesc</h4>

<p>Extracts a single subregion (a “slice”) of an input tensor.</p>

<p>The elements copied in the slice are determined using three values for each dimension.</p>
<ul>
  <li>The offset marks the first element to copy in a dimension.</li>
  <li>The size marks the number of elements to copy in a dimension.</li>
  <li>The stride indicates the element increment or step in a dimension.</li>
</ul>

<p>The provided Offsets, Sizes, and Strides must only copy elements that are 
within the bounds of the input tensor (out-of-bounds reads are not permitted). 
The Sizes of the slice must exactly match the output tensor sizes. 
In general, the elements copied are calculated as follows.</p>

\[OutputTensor[OutputCoordinates] = InputTensor[Offsets + Strides \times OutputCoordinates]\]

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the tensor to extract slices from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the tensor to write the sliced data results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dimensionCount</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The number of dimensions. This field determines the size of the <code class="language-plaintext highlighter-rouge">Offsets</code>, <code class="language-plaintext highlighter-rouge">Sizes</code>, and <code class="language-plaintext highlighter-rouge">Strides</code> arrays. This value must match the <code class="language-plaintext highlighter-rouge">DimensionCount</code> of the input and output tensors. This value must be between 1 and 8, inclusively.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">offsets</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>An array containing the slice’s start along each dimension of the input tensor, in elements.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">sizes</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>An array containing the slice’s size along each dimension, in elements. The values in this array must match the sizes specified in the output tensor.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">strides</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>An array containing the slice’s stride along each dimension of the input tensor, in elements. A stride larger than 1 indicates that elements of the input tensor may be skipped (for example, a stride of 2 will select every second element along the dimension).</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<h4 id="castoperatordesc">CastOperatorDesc</h4>

<p>Casts each element in the input to the data type of the output tensor, 
and stores the result in the corresponding element of the output.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<h4 id="splitoperatordesc">SplitOperatorDesc</h4>

<p>Splits an input tensor along an axis into multiple output tensors.</p>

<p>All input and output tensors must have the same sizes, except for the split axis. 
The size of input tensor in the split axis determines the possible splits. For example, 
if the input tensor’s split axis has size 3, then there are these potential splits:</p>

<ul>
  <li>1+1+1 (3 outputs)</li>
  <li>1+2 (2 outputs)</li>
  <li>2+1 (2 outputs)</li>
  <li>3 (1 output, which is simply a copy of the input tensor).</li>
</ul>

<p>The output tensors’ split axis sizes must sum up to exactly the input tensor’s 
split axis size.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensorCount</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>This parameter determines the size of the <code class="language-plaintext highlighter-rouge">OutputTensors</code> array. This value must be greater than 0.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensors</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>An array containing the descriptions of the tensors split off from the input tensor. The output sizes must have the same sizes as the input tensor except for the split axis.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">axis</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The index of the dimension of the input tensor to split. All input and output tensors must have identical sizes in all dimensions except for this axis. This value must be in the range [0, <code class="language-plaintext highlighter-rouge">InputTensor.DimensionCount</code> - 1].</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="joinoperatordesc">JoinOperatorDesc</h4>

<p>Concatenates an array of input tensors along a specified axis.</p>

<p>Input tensors may only be joined if their sizes are identical in all 
dimensions except for the join axis, which may contain any non-zero 
size. The output sizes are equal to the input sizes except for the 
join axis, which is the sum of all inputs’ join axis size.</p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensorCount</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>This parameter determines the size of the <code class="language-plaintext highlighter-rouge">InputTensors</code> array. This value must be greater than 0.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensors</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>An array containing the descriptions of the tensors to join into a single output tensor. All input tensors in this array must have the same sizes except for the join axis, which may have any non-zero value.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>The tensor to write the joined input tensors into. The output sizes must have the same sizes as all input tensors except for the join axis, which must be equal to the sum of all inputs’ join axis size.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">axis</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The index of the dimension of the input tensors to join. All input and output tensors must have identical sizes in all dimensions except for this axis. This value must be in the range [0, <code class="language-plaintext highlighter-rouge">OutputTensor.DimensionCount</code> - 1].</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="paddingoperatordesc">PaddingOperatorDesc</h4>

<p>Inflates the input tensor with constant or mirrored values on the edges, 
and writes the result to the output.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dimensionCount</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The size of the arrays pointed to by StartPadding and EndPadding. This value must be the same value as the dimension count of <code class="language-plaintext highlighter-rouge">InputTensor</code> and <code class="language-plaintext highlighter-rouge">OutputTensor</code>.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">startPadding</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>The sizes of the padding regions to add at the beginning of each dimension. For each dimension <code class="language-plaintext highlighter-rouge">i</code>, <code class="language-plaintext highlighter-rouge">StartPadding[i] = OutputTensor.Sizes[i] - InputTensor.Sizes[i] - EndPadding[i]</code>.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">endPadding</code></td>
      <td><code class="language-plaintext highlighter-rouge">const UInt32*</code></td>
      <td>The sizes of the padding regions to add at the end of each dimension. For each dimension <code class="language-plaintext highlighter-rouge">i</code>, <code class="language-plaintext highlighter-rouge">EndPadding[i] = OutputTensor.Sizes[i] - InputTensor.Sizes[i] - StartPadding[i]</code>.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">paddingValue</code></td>
      <td><code class="language-plaintext highlighter-rouge">float</code></td>
      <td>The padding value to use when <code class="language-plaintext highlighter-rouge">paddingMode</code> is <code class="language-plaintext highlighter-rouge">DML::PaddingMode::Constant</code>. This value is ignored for other padding modes. Note that if the DataType of the tensors is not <code class="language-plaintext highlighter-rouge">DML::TensorDataType::Float16</code> or <code class="language-plaintext highlighter-rouge">DML::TensorDataType::Float32</code>, then the value might be truncated (for example, 10.6 will become 10).</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">paddingMode</code></td>
      <td><code class="language-plaintext highlighter-rouge">DML::PaddingMode</code></td>
      <td>The padding mode to use when filling the padding regions.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<h4 id="valuescale2doperatordesc">ValueScale2DOperatorDesc</h4>

<p>Performs an element-wise scale-and-bias function</p>

\[f(x) = x \times Scale + Bias\]

<p>This operator is similar to using an ElementWiseIdentityOperatorDesc 
with a scale and bias, except that ValueScale2DOperatorDesc applies a 
different bias for each channel, rather than a single bias for the entire tensor.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from. This tensor’s dimensions should be [ BatchCount, ChannelCount, Height, Width ].</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.  This tensor’s dimensions should match the <code class="language-plaintext highlighter-rouge">inputTensor</code>’s dimensions.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scale</code></td>
      <td><code class="language-plaintext highlighter-rouge">float</code></td>
      <td>Scale value to be applied to all input values.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">channelCount</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>This parameter determines the size of the Bias array. This parameter must be set to either 1 or 3, and must also match the size of the Channel dimension of the input tensor.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">bias</code></td>
      <td><code class="language-plaintext highlighter-rouge">const float*</code></td>
      <td>An array of <code class="language-plaintext highlighter-rouge">float</code> values containing the bias term for each dimension of the input tensor.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<h4 id="upsample2doperatordesc">UpSample2DOperatorDesc</h4>

<p>Upsamples the input image, writing the result into the output 
tensor. The order of the dimensions should be NCHW 
(BatchSize, ChannelCount, Height, Width) or NCDHW (BatchSize, 
ChannelCount, Depth, Height, Width), but strides can be 
used if the data is stored in a different format. 
Unlike ResampleOperatorDesc, only the last 2 dimensions 
(height and width) can be upsampled.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from. The expected dimensions of the InputTensor are [ InputBatchCount, InputChannelCount, InputHeight, InputWidth ] for 4D, and [ InputBatchCount, InputChannelCount, InputDepth, InputHeight, InputWidth ] for 5D.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to. The expected dimensions of the OutputTensor are [ InputBatchCount, InputChannelCount, InputHeight * HeightScale, InputWidth * WidthScale ] for 4D, and [ InputBatchCount, InputChannelCount, InputDepth, InputHeight * HeightScale, InputWidth * WidthScale ] for 5D.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">scaleSize</code></td>
      <td><code class="language-plaintext highlighter-rouge">Size2D</code></td>
      <td>The width and height scales of type <code class="language-plaintext highlighter-rouge">UInt32</code> to apply when upsampling the input. <code class="language-plaintext highlighter-rouge">0 &lt; ScaleSize.Height &lt;= UINT_MAX / InputHeight</code> and <code class="language-plaintext highlighter-rouge">0 &lt; ScaleSize.Width &lt;= UINT_MAX / InputWidth</code>.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">interpolationMode</code></td>
      <td><code class="language-plaintext highlighter-rouge">DML::InterpolationMode</code></td>
      <td>Determines the kind of interpolation used to choose output pixels.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>

<h4 id="gatheroperatordesc">GatherOperatorDesc</h4>

<p>Gathers elements from the input tensor along <code class="language-plaintext highlighter-rouge">Axis</code>, using <code class="language-plaintext highlighter-rouge">IndicesTensor</code> to remap indices.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to.  The <code class="language-plaintext highlighter-rouge">DimensionCount</code> and <code class="language-plaintext highlighter-rouge">DataType</code> of this tensor must match <code class="language-plaintext highlighter-rouge">InputTensor.DimensionCount</code>. The expected <code class="language-plaintext highlighter-rouge">OutputTensor.Sizes</code> are the concatenation of the InputTensor.Sizes leading and trailing segments split at the current <code class="language-plaintext highlighter-rouge">Axis</code> with the <code class="language-plaintext highlighter-rouge">IndicesTensor.Sizes</code> inserted between.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">indicesTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>A tensor containing the indices. The DimensionCount of this tensor must match InputTensor.DimensionCount. This operator supports negative index values when using a signed integral type with this tensor. Negative indices are interpreted as being relative to the end of the axis dimension. For example, an index of -1 refers to the last element along that dimension. Invalid indices will yield incorrect outputs, but no failure will occur, and all reads will be clamped safely within the input tensor’s memory.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">axis</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The axis dimension of InputTensor to gather on, ranging [0, <code class="language-plaintext highlighter-rouge">InputTensor.DimensionCount</code>).</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">indexDimensions</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The number of actual index dimensions within the IndicesTensor after ignoring any irrelevant leading ones, ranging [0, <code class="language-plaintext highlighter-rouge">IndicesTensor.DimensionCount</code>). For example, given IndicesTensor.Sizes = [ 1, 1, 4, 6 ] and <code class="language-plaintext highlighter-rouge">IndexDimensions</code> = 3, the actual meaningful indices are [ 1, 4, 6 ].</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#baseoperatordesc"><code class="language-plaintext highlighter-rouge">BaseOperatorDesc</code></a>.</p>

<h4 id="spacetodepthoperatordesc">SpaceToDepthOperatorDesc</h4>

<p>Rearranges blocks of spatial data into depth. The operator outputs a 
copy of the input tensor where values from the height and width 
dimensions are moved to the depth dimension.</p>

<p><b>Constructor parameters:</b></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">inputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the input tensor to read from. The input tensor’s dimensions are [ Batch, Channels, Height, Width ].</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">outputTensor</code></td>
      <td><code class="language-plaintext highlighter-rouge">const TensorDesc*</code></td>
      <td>Describes the output tensor to write the results to. The output tensor’s dimensions are [ Batch, Channels / (BlockSize * BlockSize), Height * BlockSize, Width * BlockSize ].</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">blockSize</code></td>
      <td><code class="language-plaintext highlighter-rouge">UInt32</code></td>
      <td>The width and height of the Blocks that are moved.</td>
    </tr>
  </tbody>
</table>

<p>Derived from <a href="#unaryoperatordesc"><code class="language-plaintext highlighter-rouge">UnaryOperatorDesc</code></a>.</p>


  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Harlinn</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Harlinn</li><li><a class="u-email" href="mailto:espen@harlinn.no">espen@harlinn.no</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/Harlinn"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">Harlinn</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is my personal site. </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
